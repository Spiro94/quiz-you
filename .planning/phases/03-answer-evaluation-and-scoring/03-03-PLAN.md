---
phase: 03-answer-evaluation-and-scoring
plan: 03
type: execute
wave: 3
depends_on: [03-01, 03-02]
files_modified:
  - src/hooks/useAnswerEvaluation.ts
  - src/components/quiz/EvaluationResult.tsx
  - src/pages/QuizSession.tsx
autonomous: true
requirements: [QUIZ-04, EVAL-01, EVAL-02, EVAL-03, EVAL-04, EVAL-05]

must_haves:
  truths:
    - "When user clicks Submit Answer, the app immediately saves the answer to DB (pending_evaluation), then calls the LLM, then displays score/feedback/model answer — all within one user action"
    - "The UI shows 'Evaluating your answer...' with a spinner while LLM evaluates; the Submit button is disabled to prevent double-submission"
    - "At 20 seconds a 'Still evaluating...' message appears; at 30 seconds (timeout) a 'Retry' button appears"
    - "After evaluation succeeds, EvaluationResult renders: score (0-100), feedback (markdown), model answer (markdown), and a Next Question button"
    - "Clicking Next Question calls moveToNextQuestion() from QuizContext and hides the EvaluationResult panel"
    - "If evaluation fails after 3 retries, the error is shown with a Retry button; the answer row in DB stays as 'pending_evaluation'"
  artifacts:
    - path: "src/hooks/useAnswerEvaluation.ts"
      provides: "Hook managing evaluation lifecycle: saving answer, calling LLM, updating DB, tracking state"
      exports: ["useAnswerEvaluation"]
    - path: "src/components/quiz/EvaluationResult.tsx"
      provides: "Component displaying score ring/number, feedback (markdown-it), model answer (markdown-it), Next Question button"
      exports: ["EvaluationResult"]
    - path: "src/pages/QuizSession.tsx"
      provides: "handleSubmit wired to useAnswerEvaluation; evaluation state toggles between AnswerInput and EvaluationResult"
  key_links:
    - from: "src/hooks/useAnswerEvaluation.ts"
      to: "src/lib/quiz/answers.ts"
      via: "insertAnswer() then updateAnswerEvaluation()"
      pattern: "insertAnswer|updateAnswerEvaluation"
    - from: "src/hooks/useAnswerEvaluation.ts"
      to: "src/lib/llm/evaluation.ts"
      via: "evaluateWithRetry(params)"
      pattern: "evaluateWithRetry"
    - from: "src/pages/QuizSession.tsx"
      to: "src/hooks/useAnswerEvaluation.ts"
      via: "const { submitAnswer, evaluating, evaluation, error } = useAnswerEvaluation()"
      pattern: "useAnswerEvaluation"
    - from: "src/pages/QuizSession.tsx"
      to: "src/components/quiz/EvaluationResult.tsx"
      via: "conditional render: evaluation ? <EvaluationResult /> : <AnswerInput />"
      pattern: "EvaluationResult"
---

<objective>
Wire the answer submission flow end-to-end: hook manages the full lifecycle (save to DB → evaluate → update DB → expose result state), EvaluationResult component renders the feedback, QuizSession page replaces the AnswerInput with EvaluationResult after submission.

Purpose: This is the user-facing payoff — what they see after submitting an answer. The hook encapsulates all the error states (timeout, retry, failure) so QuizSession stays clean. The EvaluationResult component uses markdown-it (html:false, same as QuestionDisplay) to safely render LLM markdown.

Output: useAnswerEvaluation hook, EvaluationResult component, QuizSession.tsx wired to the real evaluation pipeline.
</objective>

<execution_context>
@/Users/danielvillamizar/.claude/get-shit-done/workflows/execute-plan.md
@/Users/danielvillamizar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-answer-evaluation-and-scoring/03-RESEARCH.md
@.planning/phases/03-answer-evaluation-and-scoring/03-01-SUMMARY.md
@.planning/phases/03-answer-evaluation-and-scoring/03-02-SUMMARY.md

<!-- Existing code to integrate with -->
@src/pages/QuizSession.tsx
@src/context/QuizContext.tsx
@src/hooks/useQuestionGeneration.ts
@src/components/quiz/AnswerInput.tsx
@src/components/quiz/QuestionDisplay.tsx
@src/lib/quiz/answers.ts
@src/lib/llm/evaluation.ts
@src/lib/llm/types.ts
@src/types/database.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create useAnswerEvaluation hook</name>
  <files>src/hooks/useAnswerEvaluation.ts</files>
  <action>
    Create src/hooks/useAnswerEvaluation.ts. This hook manages the full atomic answer submission lifecycle and exposes clean state to QuizSession.

    The hook must:
    1. Accept sessionId, questionId (nullable), questionIndex, and the GeneratedQuestion object
    2. On submitAnswer(userAnswer): insert answer to DB first (insertAnswer), then evaluate (evaluateWithRetry), then update DB (updateAnswerEvaluation), or mark failed (markAnswerEvaluationFailed) if all retries exhausted
    3. Track: evaluating (boolean), evaluation (EvaluationResult | null), error (string | null), elapsedSeconds (number, increments every second during evaluation to drive the "Still evaluating..." UI)
    4. Expose: submitAnswer(answer: string), evaluating, evaluation, error, elapsedSeconds, reset (clears evaluation for next question)

    ```typescript
    // src/hooks/useAnswerEvaluation.ts
    // Manages the full answer submission lifecycle:
    //   1. insertAnswer() → pending_evaluation row (DATA-01)
    //   2. evaluateWithRetry() → LLM result with G-Eval (EVAL-01/02/03/04)
    //   3. updateAnswerEvaluation() → completed row (EVAL-05)
    // Exposes evaluation state to QuizSession page.
    import { useState, useCallback, useRef, useEffect } from 'react'
    import { insertAnswer, updateAnswerEvaluation, markAnswerEvaluationFailed } from '../lib/quiz/answers'
    import { evaluateWithRetry } from '../lib/llm/evaluation'
    import type { EvaluationResult } from '../lib/llm/types'
    import type { GeneratedQuestion } from '../types/quiz'

    interface UseAnswerEvaluationParams {
      sessionId: string
      questionIndex: number
      question: GeneratedQuestion
      questionId?: string | null   // DB ID of the question row (if saved to quiz_questions)
    }

    interface UseAnswerEvaluationReturn {
      submitAnswer: (userAnswer: string) => Promise<void>
      evaluating: boolean
      evaluation: EvaluationResult | null
      error: string | null
      elapsedSeconds: number    // Drive "Still evaluating..." at 20s, retry UI at 30s
      reset: () => void         // Call before moving to next question
    }

    export function useAnswerEvaluation(params: UseAnswerEvaluationParams): UseAnswerEvaluationReturn {
      const [evaluating, setEvaluating] = useState(false)
      const [evaluation, setEvaluation] = useState<EvaluationResult | null>(null)
      const [error, setError] = useState<string | null>(null)
      const [elapsedSeconds, setElapsedSeconds] = useState(0)
      const timerRef = useRef<ReturnType<typeof setInterval> | null>(null)

      // Start/stop the elapsed seconds counter
      useEffect(() => {
        if (evaluating) {
          setElapsedSeconds(0)
          timerRef.current = setInterval(() => {
            setElapsedSeconds(prev => prev + 1)
          }, 1000)
        } else {
          if (timerRef.current) {
            clearInterval(timerRef.current)
            timerRef.current = null
          }
        }
        return () => {
          if (timerRef.current) clearInterval(timerRef.current)
        }
      }, [evaluating])

      const submitAnswer = useCallback(async (userAnswer: string) => {
        setEvaluating(true)
        setError(null)
        setEvaluation(null)

        let answerId: string | null = null

        try {
          // Step 1: Save answer to DB before LLM call (DATA-01, DATA-03)
          const answerRow = await insertAnswer({
            sessionId: params.sessionId,
            questionId: params.questionId ?? null,
            questionIndex: params.questionIndex,
            userAnswer
          })
          answerId = answerRow.id

          // Step 2: Evaluate with G-Eval + retry + timeout (EVAL-01/02/03/04)
          // STATELESS: evaluateWithRetry builds fresh prompt from question/answer only
          const result = await evaluateWithRetry({
            question: `${params.question.title}\n\n${params.question.body}`,
            questionType: params.question.type,
            difficulty: params.question.difficulty,
            topic: params.question.topic,
            userAnswer,
            expectedFormat: params.question.expectedFormat
          })

          // Step 3: Persist evaluation result (EVAL-05)
          await updateAnswerEvaluation(answerId, {
            score: result.score,
            reasoning: result.reasoning,
            feedback: result.feedback,
            modelAnswer: result.modelAnswer
          })

          setEvaluation(result)
        } catch (err) {
          const message = err instanceof Error ? err.message : 'Evaluation failed'
          setError(message)

          // Mark the answer as evaluation_failed so Phase 4 can show it in history
          if (answerId) {
            markAnswerEvaluationFailed(answerId).catch(() => {
              // Best-effort — don't throw if status update also fails
            })
          }
        } finally {
          setEvaluating(false)
        }
      }, [params])

      const reset = useCallback(() => {
        setEvaluation(null)
        setError(null)
        setElapsedSeconds(0)
      }, [])

      return { submitAnswer, evaluating, evaluation, error, elapsedSeconds, reset }
    }
    ```
  </action>
  <verify>
    Run: `npx tsc --noEmit` — zero errors.
    Check: `submitAnswer` is async and calls insertAnswer, then evaluateWithRetry, then updateAnswerEvaluation in sequence.
    Check: `elapsedSeconds` increments every second while evaluating (driven by setInterval, cleared on completion).
    Check: `reset()` clears evaluation and error state.
  </verify>
  <done>
    useAnswerEvaluation hook exported. submitAnswer implements the 3-step atomic pattern (insert → evaluate → update). elapsedSeconds increments during evaluation. Error caught and passed to error state; answer marked evaluation_failed in DB. Zero TypeScript errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build EvaluationResult component and wire QuizSession</name>
  <files>
    src/components/quiz/EvaluationResult.tsx
    src/pages/QuizSession.tsx
  </files>
  <action>
    Create src/components/quiz/EvaluationResult.tsx. Use markdown-it with html:false (same as QuestionDisplay.tsx). Display: score with color coding, feedback, model answer, Next Question button.

    ```typescript
    // src/components/quiz/EvaluationResult.tsx
    // Displays LLM evaluation: score (0-100), feedback (markdown), model answer (markdown).
    // Uses markdown-it with html:false — same pattern as QuestionDisplay to prevent XSS.
    import MarkdownIt from 'markdown-it'
    import type { EvaluationResult as EvaluationResultType } from '../../lib/llm/types'

    const md = new MarkdownIt({ html: false })

    interface EvaluationResultProps {
      evaluation: EvaluationResultType
      onNext: () => void
      isLastQuestion: boolean
    }

    function getScoreColor(score: number): string {
      if (score >= 85) return 'text-green-600'
      if (score >= 70) return 'text-blue-600'
      if (score >= 50) return 'text-yellow-600'
      return 'text-red-600'
    }

    function getScoreLabel(score: number): string {
      if (score >= 85) return 'Excellent!'
      if (score >= 70) return 'Good work'
      if (score >= 50) return 'Needs improvement'
      return 'Keep practicing'
    }

    export function EvaluationResult({ evaluation, onNext, isLastQuestion }: EvaluationResultProps) {
      return (
        <div className="space-y-6">
          {/* Score */}
          <div className="text-center py-4">
            <div className={`text-6xl font-bold tabular-nums ${getScoreColor(evaluation.score)}`}>
              {evaluation.score}
            </div>
            <div className="text-sm text-gray-500 mt-1">out of 100</div>
            <div className={`text-base font-medium mt-2 ${getScoreColor(evaluation.score)}`}>
              {getScoreLabel(evaluation.score)}
            </div>
          </div>

          {/* Feedback */}
          <div className="rounded-lg border-l-4 border-blue-500 bg-blue-50 p-4">
            <h3 className="text-sm font-semibold text-blue-900 mb-2">Feedback</h3>
            <div
              className="prose prose-sm max-w-none text-blue-800 prose-code:bg-blue-100 prose-pre:overflow-x-auto"
              dangerouslySetInnerHTML={{ __html: md.render(evaluation.feedback) }}
            />
          </div>

          {/* Model Answer */}
          <div className="rounded-lg border-l-4 border-green-500 bg-green-50 p-4">
            <h3 className="text-sm font-semibold text-green-900 mb-2">Model Answer</h3>
            <div
              className="prose prose-sm max-w-none text-green-800 prose-code:bg-green-100 prose-pre:overflow-x-auto [&_pre]:whitespace-pre-wrap [&_code]:break-words"
              dangerouslySetInnerHTML={{ __html: md.render(evaluation.modelAnswer) }}
            />
          </div>

          {/* Navigation */}
          <button
            onClick={onNext}
            className="w-full rounded-md bg-blue-600 px-4 py-3 text-sm font-semibold text-white shadow-sm hover:bg-blue-500 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 transition-colors"
          >
            {isLastQuestion ? 'Finish Quiz' : 'Next Question'}
          </button>
        </div>
      )
    }
    ```

    Then update src/pages/QuizSession.tsx — replace the stub handleSubmit and add evaluation flow:

    1. Import useAnswerEvaluation, EvaluationResult
    2. Add useAnswerEvaluation hook call (pass current question from QuizContext)
    3. Replace stub handleSubmit: call `submitAnswer(answer)` from the hook
    4. Conditionally render: if `evaluation` is set, show EvaluationResult instead of AnswerInput + QuestionDisplay answer area
    5. Wire EvaluationResult's onNext: call `reset()` from hook, then `moveToNextQuestion()`
    6. Add evaluation loading state UI between AnswerInput and EvaluationResult:
       - `evaluating && elapsedSeconds < 20`: spinner + "Evaluating your answer..."
       - `evaluating && elapsedSeconds >= 20`: spinner + "Still evaluating... (taking longer than usual)"
       - After timeout (caught as error): show error with Retry button that re-calls submitAnswer with same answer

    The question lookup: `session?.questions[session.currentQuestionIndex]` — this is the GeneratedQuestion.
    The questionId lookup: need to pass null for now (quiz_questions rows are not returned with IDs in Phase 2 — Phase 4 can backfill). Pass `question_id: null` via useAnswerEvaluation.

    Key change to handleSubmit — currently in QuizSession.tsx:
    ```typescript
    const handleSubmit = (answer: string) => {
      console.log('Answer submitted (evaluation in Phase 3):', answer.substring(0, 100))
      moveToNextQuestion()
    }
    ```

    Replace with:
    ```typescript
    const handleSubmit = async (answer: string) => {
      await submitAnswer(answer)
      // QuizContext.moveToNextQuestion() is called by EvaluationResult's onNext button, not here
    }
    ```

    Store the last submitted answer in a ref (useRef) so the Retry button can re-submit: `lastAnswerRef.current = answer` before calling submitAnswer.

    Also update AnswerInput's onSubmit prop type in this file — it may need to accept a Promise return (check AnswerInput.tsx's interface; update if needed).

    isLastQuestion calculation: `session ? session.currentQuestionIndex >= session.totalQuestions - 1 : false`
  </action>
  <verify>
    Run: `npx tsc --noEmit` — zero errors.
    Run: `npm run build` — builds without errors (no chunk size errors are blockers; warnings are acceptable).
    Check: `EvaluationResult` component renders score, feedback section, model answer section, and next button.
    Check: QuizSession.tsx imports `useAnswerEvaluation` and `EvaluationResult`.
    Check: QuizSession.tsx no longer has the Phase 2 stub `console.log('Answer submitted (evaluation in Phase 3)')`.
  </verify>
  <done>
    EvaluationResult component created with score color-coding (green/blue/yellow/red), markdown-it-rendered feedback and model answer (html:false), and Next/Finish button. QuizSession.tsx wired to useAnswerEvaluation — handleSubmit calls submitAnswer(). After evaluation, EvaluationResult replaces AnswerInput. onNext calls reset() + moveToNextQuestion(). Evaluation loading states at 0s and 20s shown. Zero TypeScript errors, build passes.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — zero errors.
2. `npm run build` — succeeds (chunk warnings are acceptable, chunk errors are not).
3. `src/hooks/useAnswerEvaluation.ts` exports `useAnswerEvaluation` with `submitAnswer`, `evaluating`, `evaluation`, `error`, `elapsedSeconds`, `reset`.
4. `src/components/quiz/EvaluationResult.tsx` renders score number, feedback div, model answer div, and Next Question/Finish Quiz button.
5. `src/pages/QuizSession.tsx` conditionally renders `EvaluationResult` when `evaluation !== null`, and `AnswerInput` otherwise.
6. `src/pages/QuizSession.tsx` shows spinner with "Evaluating your answer..." when `evaluating === true`.
7. Retry button appears when `error` is set, re-calls submitAnswer with the last submitted answer.
</verification>

<success_criteria>
- useAnswerEvaluation hook: 3-step atomic flow (insertAnswer → evaluateWithRetry → updateAnswerEvaluation) wired correctly
- EvaluationResult shows score (colored by tier: green/blue/yellow/red), feedback (markdown), model answer (markdown)
- "Still evaluating..." appears after 20 seconds during active evaluation
- Retry button appears on evaluation failure, re-submits the same answer
- Next Question button calls reset() + moveToNextQuestion() — hides EvaluationResult and shows next question
- "Finish Quiz" label on last question (isLastQuestion === true)
- Phase 2 stub handleSubmit removed from QuizSession.tsx
- Zero TypeScript errors, build passes
</success_criteria>

<output>
After completion, create `.planning/phases/03-answer-evaluation-and-scoring/03-03-SUMMARY.md` following the template at `/Users/danielvillamizar/.claude/get-shit-done/templates/summary.md`.
</output>
