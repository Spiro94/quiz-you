---
phase: 03-answer-evaluation-and-scoring
plan: 02
type: execute
wave: 2
depends_on: [03-01]
files_modified:
  - src/lib/llm/types.ts
  - src/lib/llm/prompts.ts
  - src/lib/llm/evaluation.ts
  - src/lib/llm/claude.ts
autonomous: true
requirements: [EVAL-01, EVAL-02, EVAL-03, EVAL-04]

must_haves:
  truths:
    - "The evaluation service produces a score (0-100), feedback, model answer, and chain-of-thought reasoning for every answer submitted"
    - "Evaluation uses the G-Eval pattern: LLM generates reasoning steps before committing to a numeric score"
    - "Each evaluation call is stateless — no context from previous answers is carried over, preventing context window degradation"
    - "A 30-second timeout enforces EVAL-04; failed evaluations retry up to 3 times with exponential backoff (1s, 2s, 4s)"
    - "LLM output is validated with Zod schema — if the response is malformed, the retry fires instead of crashing"
    - "Temperature is set to 0.2 for deterministic scoring; rubric is embedded in every prompt"
    - "The ClaudeProvider gains an evaluateAnswer() method that accepts EvaluationParams and returns EvaluationResult"
  artifacts:
    - path: "src/lib/llm/types.ts"
      provides: "EvaluationParams and EvaluationResult interfaces added to LLMProvider contract"
      exports: ["EvaluationParams", "EvaluationResult"]
    - path: "src/lib/llm/evaluation.ts"
      provides: "evaluateWithRetry() — G-Eval implementation with timeout + backoff + Zod validation"
      exports: ["evaluateWithRetry", "EvaluationSchema"]
    - path: "src/lib/llm/prompts.ts"
      provides: "buildEvaluationPrompt() with G-Eval chain-of-thought structure and per-difficulty rubric"
      exports: ["buildEvaluationPrompt", "EVAL_PROMPT_VERSION"]
    - path: "src/lib/llm/claude.ts"
      provides: "ClaudeProvider.evaluateAnswer() using temperature=0.2 and max_tokens=2048"
  key_links:
    - from: "src/lib/llm/evaluation.ts"
      to: "src/lib/llm/claude.ts"
      via: "getLLMProvider().evaluateAnswer()"
      pattern: "evaluateAnswer\\("
    - from: "src/lib/llm/evaluation.ts"
      to: "src/lib/llm/prompts.ts"
      via: "buildEvaluationPrompt(params)"
      pattern: "buildEvaluationPrompt"
    - from: "src/lib/llm/types.ts"
      to: "LLMProvider interface"
      via: "evaluateAnswer method on LLMProvider"
      pattern: "evaluateAnswer.*EvaluationParams.*EvaluationResult"
---

<objective>
Build the G-Eval evaluation service — the core scoring engine. Implements chain-of-thought scoring with stateless context, multi-dimensional rubrics, Zod output validation, 30-second timeout, and exponential backoff retry.

Purpose: Evaluation accuracy is the single point of failure for user trust. This plan installs the safeguards: rubric-anchored scoring prevents variance, stateless design prevents context degradation, Zod validation catches malformed LLM output before it reaches the UI.

Output: EvaluationParams/EvaluationResult types, G-Eval prompt, evaluation service with retry, ClaudeProvider.evaluateAnswer() method.
</objective>

<execution_context>
@/Users/danielvillamizar/.claude/get-shit-done/workflows/execute-plan.md
@/Users/danielvillamizar/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-answer-evaluation-and-scoring/03-RESEARCH.md
@.planning/phases/03-answer-evaluation-and-scoring/03-01-SUMMARY.md

<!-- Existing LLM layer to extend -->
@src/lib/llm/types.ts
@src/lib/llm/claude.ts
@src/lib/llm/prompts.ts
@src/lib/llm/index.ts
@src/types/quiz.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add EvaluationParams/EvaluationResult types and buildEvaluationPrompt</name>
  <files>
    src/lib/llm/types.ts
    src/lib/llm/prompts.ts
  </files>
  <action>
    Update src/lib/llm/types.ts — add EvaluationParams, EvaluationResult, and extend LLMProvider:

    ```typescript
    // Add after the existing QuestionGenerationParams and LLMProvider definitions:

    export interface EvaluationParams {
      question: string            // Full question body (title + body concatenated)
      questionType: 'coding' | 'theoretical'
      difficulty: 'beginner' | 'normal' | 'advanced'
      topic: string
      userAnswer: string
      expectedFormat?: string     // e.g. "Python function", "Paragraph explanation"
    }

    export interface EvaluationResult {
      reasoning: string           // G-Eval chain-of-thought (saved to DB, not shown to user in v1)
      score: number               // 0-100
      feedback: string            // Markdown-formatted feedback shown to user
      modelAnswer: string         // Markdown-formatted model/reference answer shown to user
    }
    ```

    Also extend the LLMProvider interface to include evaluateAnswer:
    ```typescript
    export interface LLMProvider {
      generateQuestion(params: QuestionGenerationParams): Promise<string>
      generateQuestionStream(params: QuestionGenerationParams): AsyncIterable<string>
      evaluateAnswer(params: EvaluationParams): Promise<EvaluationResult>
    }
    ```

    Then update src/lib/llm/prompts.ts — add EVAL_PROMPT_VERSION and buildEvaluationPrompt after the existing buildQuestionPrompt function:

    ```typescript
    export const EVAL_PROMPT_VERSION = 'v1.0'

    export function buildEvaluationPrompt(params: EvaluationParams): string {
      const rubric = buildRubric(params.difficulty, params.questionType)
      const formatHint = params.expectedFormat
        ? `\nExpected format: ${params.expectedFormat}`
        : ''

      return `You are an expert technical interviewer evaluating a candidate's answer. Be rigorous but fair.

    QUESTION (${params.topic} — ${params.difficulty}):
    ${params.question}${formatHint}

    CANDIDATE'S ANSWER:
    ${params.userAnswer || '[No answer provided]'}

    SCORING RUBRIC:
    ${rubric}

    EVALUATION PROCESS — follow these steps in order:
    1. Correctness: Is the answer technically accurate? Identify any errors.
    2. Completeness: Does it address all parts of the question?
    3. Quality: Is the explanation clear and at the appropriate depth for ${params.difficulty} level?
    4. Presentation: ${params.questionType === 'coding' ? 'Is the code clean, readable, and efficient?' : 'Is it well-structured with clear reasoning?'}

    Return ONLY a valid JSON object — no markdown fences, no explanation outside the JSON:
    {
      "reasoning": "Your step-by-step analysis following the 4 evaluation steps above",
      "score": <integer 0-100 matching the rubric>,
      "feedback": "Markdown-formatted feedback: what was good, what to improve, and specific suggestions",
      "modelAnswer": "Markdown-formatted model answer the candidate can learn from"
    }

    Evaluation prompt version: ${EVAL_PROMPT_VERSION}`
    }

    function buildRubric(difficulty: 'beginner' | 'normal' | 'advanced', type: 'coding' | 'theoretical'): string {
      const rubrics = {
        beginner: {
          coding: '0-30: Syntax errors or logic is fundamentally wrong. 31-69: Mostly works but has notable bugs or misses key concepts. 70-84: Correct with minor issues. 85-100: Clean, correct solution demonstrating solid fundamentals.',
          theoretical: '0-30: Incorrect or missing key concepts. 31-69: Partially correct, significant gaps in understanding. 70-84: Correct with minor omissions. 85-100: Comprehensive, accurate, well-explained.'
        },
        normal: {
          coding: '0-30: Wrong approach or major logic bugs. 31-69: Works but inefficient, unclear, or misses real-world considerations. 70-84: Good solution, minor optimization opportunity. 85-100: Excellent code quality, efficiency, and clarity.',
          theoretical: '0-30: Fundamental misunderstanding. 31-69: Correct basics but missing important nuance or real-world context. 70-84: Good depth with minor gaps. 85-100: Expert-level insight with clear reasoning.'
        },
        advanced: {
          coding: '0-30: Fails to solve or contains critical bugs. 31-69: Solves but misses edge cases, performance considerations, or best practices. 70-84: Solid solution, one advanced aspect missed. 85-100: Handles edge cases, optimal complexity, production-ready.',
          theoretical: '0-30: Incorrect or superficial. 31-69: Addresses the question but lacks depth or misses key tradeoffs. 70-84: Strong depth, one advanced consideration missing. 85-100: Expert analysis including tradeoffs, real-world implications, and nuance.'
        }
      }
      return rubrics[difficulty][type]
    }
    ```

    Import EvaluationParams in prompts.ts: `import type { EvaluationParams } from './types'`
  </action>
  <verify>
    Run: `npx tsc --noEmit` — zero errors.
    Check: `EvaluationParams`, `EvaluationResult` exported from `src/lib/llm/types.ts`.
    Check: `LLMProvider` interface now includes `evaluateAnswer(params: EvaluationParams): Promise<EvaluationResult>`.
    Check: `buildEvaluationPrompt` and `EVAL_PROMPT_VERSION` exported from `src/lib/llm/prompts.ts`.
  </verify>
  <done>
    EvaluationParams and EvaluationResult types added to types.ts. LLMProvider interface extended with evaluateAnswer. buildEvaluationPrompt exported from prompts.ts with per-difficulty/per-type rubric. Zero TypeScript errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build evaluation service with G-Eval, retry, and Zod validation, and update ClaudeProvider</name>
  <files>
    src/lib/llm/evaluation.ts
    src/lib/llm/claude.ts
  </files>
  <action>
    Create src/lib/llm/evaluation.ts:

    ```typescript
    // src/lib/llm/evaluation.ts
    // G-Eval evaluation service with timeout, exponential backoff, and Zod output validation.
    // STATELESS: every call gets a fresh prompt — no context from previous answers (STATE.md requirement).
    // Retry logic: 3 attempts, delays 1s → 2s → 4s with 10% jitter. Covers transient API failures.
    import { z } from 'zod'
    import { getLLMProvider } from './index'
    import { buildEvaluationPrompt } from './prompts'
    import type { EvaluationParams, EvaluationResult } from './types'

    // Zod schema for validating LLM evaluation output
    export const EvaluationSchema = z.object({
      reasoning: z.string().min(10, 'Reasoning too short'),
      score: z.number().int().min(0).max(100),
      feedback: z.string().min(10, 'Feedback too short'),
      modelAnswer: z.string().min(10, 'Model answer too short')
    })

    const EVALUATION_TIMEOUT_MS = 30_000
    const MAX_RETRIES = 3
    const BASE_DELAY_MS = 1_000

    // Top-level evaluation entry point used by useAnswerEvaluation hook (Plan 03-03).
    // Wraps evaluateWithRetry; exported for direct use.
    export async function evaluateWithRetry(params: EvaluationParams): Promise<EvaluationResult> {
      let lastError: Error = new Error('Evaluation failed')

      for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
        try {
          return await evaluateWithTimeout(params)
        } catch (err) {
          lastError = err instanceof Error ? err : new Error(String(err))

          if (attempt < MAX_RETRIES - 1) {
            const delay = BASE_DELAY_MS * Math.pow(2, attempt)
            const jitter = Math.random() * delay * 0.1
            await new Promise(resolve => setTimeout(resolve, delay + jitter))
          }
        }
      }

      throw new Error(`Evaluation failed after ${MAX_RETRIES} attempts: ${lastError.message}`)
    }

    async function evaluateWithTimeout(params: EvaluationParams): Promise<EvaluationResult> {
      return Promise.race([
        runEvaluation(params),
        new Promise<never>((_, reject) =>
          setTimeout(
            () => reject(new Error(`Evaluation timed out after ${EVALUATION_TIMEOUT_MS / 1000}s`)),
            EVALUATION_TIMEOUT_MS
          )
        )
      ])
    }

    async function runEvaluation(params: EvaluationParams): Promise<EvaluationResult> {
      const provider = getLLMProvider()
      // STATELESS: each call uses buildEvaluationPrompt with only the current question/answer.
      // Never pass previous evaluation results as context.
      const result = await provider.evaluateAnswer(params)

      // Zod validates the structured output — catches "score": "seventy-five" style failures
      const parsed = EvaluationSchema.parse(result)
      return parsed
    }
    ```

    Then update src/lib/llm/claude.ts — add the evaluateAnswer method to ClaudeProvider (implementing the LLMProvider contract from Task 1). Add after generateQuestionStream:

    ```typescript
    async evaluateAnswer(params: EvaluationParams): Promise<EvaluationResult> {
      const prompt = buildEvaluationPrompt(params)
      const message = await this.client.messages.create({
        model: 'claude-opus-4-6',
        max_tokens: 2048,        // Evaluation needs more tokens than question generation
        temperature: 0.2,        // Low temperature for deterministic, consistent scoring
        messages: [{ role: 'user', content: prompt }]
      })

      const content = message.content[0]
      if (content.type !== 'text') throw new Error('Unexpected LLM response type from evaluation')

      // Parse JSON — LLM is prompted to return only valid JSON
      let parsed: unknown
      try {
        parsed = JSON.parse(content.text)
      } catch {
        throw new Error(`Evaluation returned invalid JSON: ${content.text.substring(0, 200)}`)
      }

      // EvaluationSchema.parse is called again in evaluation.ts, but we validate here too
      // to catch LLM provider-level issues early with a clear error
      return parsed as EvaluationResult
    }
    ```

    Add required imports to claude.ts: `import type { EvaluationParams, EvaluationResult } from './types'` and `import { buildEvaluationPrompt } from './prompts'`.

    Also update the OpenAI provider (src/lib/llm/openai.ts) if it exists — add a stub evaluateAnswer that throws 'OpenAI evaluation not implemented' to satisfy the LLMProvider interface. Check if openai.ts implements LLMProvider; if so, add the stub.
  </action>
  <verify>
    Run: `npx tsc --noEmit` — zero errors (LLMProvider interface now requires evaluateAnswer; ClaudeProvider must implement it).
    Check: `src/lib/llm/evaluation.ts` exports `evaluateWithRetry` and `EvaluationSchema`.
    Check: `ClaudeProvider` has `evaluateAnswer` method with `temperature: 0.2` and `max_tokens: 2048`.
    Check: OpenAI provider satisfies LLMProvider interface (stub or implementation).
  </verify>
  <done>
    evaluateWithRetry exported from evaluation.ts — implements G-Eval (LLM generates reasoning before score), retries up to 3 times with exponential backoff + jitter, enforces 30s timeout. ClaudeProvider.evaluateAnswer() calls buildEvaluationPrompt and parses JSON response. EvaluationSchema validates output shape. Zero TypeScript errors.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — zero errors after both tasks complete.
2. `src/lib/llm/types.ts` — LLMProvider interface contains `evaluateAnswer(params: EvaluationParams): Promise<EvaluationResult>`.
3. `src/lib/llm/evaluation.ts` — `evaluateWithRetry` applies 3-attempt retry with 1s/2s/4s+jitter delays and 30s timeout.
4. `src/lib/llm/claude.ts` — `evaluateAnswer` uses `temperature: 0.2` and `max_tokens: 2048`.
5. `src/lib/llm/prompts.ts` — `buildEvaluationPrompt` includes difficulty-specific rubric and G-Eval 4-step instructions.
6. EvaluationSchema uses `z.number().int().min(0).max(100)` — rejects non-integer and out-of-range scores.
</verification>

<success_criteria>
- EvaluationParams and EvaluationResult interfaces defined and exported
- LLMProvider interface extended with evaluateAnswer — no TypeScript errors
- G-Eval prompt built with stateless design (no prior context) and multi-dimensional rubric (correctness, completeness, quality, presentation)
- 30-second timeout enforced on every evaluation call
- 3-attempt exponential backoff retry (1s, 2s, 4s + 10% jitter)
- Zod EvaluationSchema validates score is integer 0-100, feedback and modelAnswer are non-empty strings
- ClaudeProvider.evaluateAnswer() sets temperature=0.2 and max_tokens=2048
- OpenAI provider satisfies updated LLMProvider interface
- Zero TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-answer-evaluation-and-scoring/03-02-SUMMARY.md` following the template at `/Users/danielvillamizar/.claude/get-shit-done/templates/summary.md`.
</output>
